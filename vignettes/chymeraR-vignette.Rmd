---
title: "chymeraR-vignette"
author: "Henry Ward"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{chymeraR-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```

## Introduction

The chymeraR package contains all the computational tools you need to process, score and analyze combinatorial CRISPR screening data. 

This document will guide you through the complete process of scoring a published combinatorial screening dataset. This dataset comprises combinatorial CRISPR screens performed with the CHyMErA experimental platform in two different cell lines, HAP1 and RPE1, across a wild-type condition and a Torin1-treated condition, with read counts taken at two different timepoints for each cell line (T12 and T18 for HAP1, T18 and T24 for RPE1). Each guide in the dataset, represented by a single row, has associated read counts for the knockout of regions specified by a Cas9 sequence and a Cas12a sequence. The first set of columns in the dataset contain metadata for guide pairs, and the remaining numeric columns contain raw read counts for the guide pairs across every screen.

The guide library used to generate this data contains several different types of guides. For this vignette, we will be interested inscoring guides that target the same gene twice ("dual-targeting" guides) and guides that target each gene of a paralogous gene pair separately ("paralog" guides). Additionally, scoring the paralog guides requires comparisons against a third set of guides that target a gene's exonic region in addition to a relatively distant intergenic region ("single-targeting" guides).

In-depth descriptions of this dataset and how it was originally scored are available in [Gonatopoulos-Pournatzis et al.](https://www.nature.com/articles/s41587-020-0437-z).

### Important publications

Please refer to the following publications for more information on the CHyMErA experimental platform, CRISPR screens and scoring them, or alternative approaches for scoring combinatorial CRISPR screening data.

[Gonatopoulos-Pournatzis et al.](https://www.nature.com/articles/s41587-020-0437-z)

max's scoring paper
flex

gemini for an alternative scoring approach?

CRISPR screen overview papers?

### Prerequisites

To follow this vignette, familiarity with CRISPR screening technology is strongly recommended. Familiarity with combinatorial CRISPR screening platforms or other ways to score CRISPR screening data is recommended, but not required.

The only additional package needed to follow this vignette is ggplot2.
```{r}
install.packages("ggplot2")
```

## Walkthrough

### Setting up

First, we want to load the chymera package, ggplot2, and the aforementioned dataset into our workspace.

```{r}
library(chymeraR)
library(ggplot2)
```

Next, let's rename the aforementioned dataset that comes bundled with chymeraR for brevity. At the same time, let's also create output folders to hold QC plots and our final results.

```{r}
# Renames dataset
df <- chymera_paralog

# Makes output folders if nonexistent
output_folder <- file.path("output", "vignette")
qc_folder <- file.path(output_folder, "qc")
if (!dir.exists(output_folder)) { dir.create(output_folder, recursive = TRUE) }
if (!dir.exists(qc_folder)) { dir.create(qc_folder) }
```

We need to put together a list of screens so that the rest of the pipeline knows which technical replicates belong to which screen. To do this, we call the `add_screen` function. This requires you to give each screen a name - for instance, HAP1_T12 - and a list of column names in the dataset of interest that belong to the screen. We first want to list names for our T0 screens, which have no technical replicates. For subsequent calls to `add_screen`, we can pass previous results in as the first argument to build up the list of screens, and will no longer have to call parameters by name.

```{r}
screens <- add_screen(name = "HAP1_T0", replicates = "HAP1.T0")
screens <- add_screen(screens, "RPE1_T0", "RPE1.T0")
```

Because we want to normalize the rest of the screens to their respective T0 screens to get log fold-changes (LFCs), we add the name of the screen that we want to normalize to in the final parameter of the `add_screen` function (the `normalize_name` parameter). All of these screens from later timepoints also have three technical replicates, A, B, and C, which are separately normalized to T0s and are automatically averaged farther downstream in the pipeline before computing results. 

```{r}
screens <- add_screen(screens, "HAP1_T12", c("HAP1.T12A", "HAP1.T12B", "HAP1.T12C"), "HAP1_T0")
screens <- add_screen(screens, "HAP1_T18", c("HAP1.T18A", "HAP1.T18B", "HAP1.T18C"), "HAP1_T0")
screens <- add_screen(screens, "Torin_T12", c("HAP1.Torin.T12A", "HAP1.Torin.T12B", "HAP1.Torin.T12C"), "HAP1_T0")
screens <- add_screen(screens, "Torin_T18", c("HAP1.Torin.T18A", "HAP1.Torin.T18B", "HAP1.Torin.T18C"), "HAP1_T0")
screens <- add_screen(screens, "RPE1_T18", c("RPE1.T18A", "RPE1.T18B", "RPE1.T18C"), "RPE1_T0")
screens <- add_screen(screens, "RPE1_T24", c("RPE1.T24A", "RPE1.T24B", "RPE1.T24C"), "RPE1_T0")
```

### Processing and QC

The first thing we want to do for any experiment is generate plots of raw read counts for all technical replicates. The easiest way to do this is to make histograms of log-scaled read counts for each replicate. A single function in chymeraR, `plot_screen_reads`, takes care of this for all screens. Like other plotting functions that output multiple plots in chymeraR, `plot_screen_reads` requires you to specify a folder to save plots to. Because these are QC plots, we will output them to the previously-created QC folder. 

```{r}
plot_screen_reads(df, screens, qc_folder)
```

Now we need to normalize each screen in three different ways: 

1. To their respective T0 screens (getting LFCs)
2. To the respective depth of each technical replicate
3. By removing guides that are too lowly or highly-expressed at T0

One workhorse function, `normalize_screens`, automatically performs all of these normalization steps. The function infers which columns of `df` need to be normalized to which T0 screens based on the `normalize_name` parameter of each screen in `screens` (screens without this optional parameter will not be normalized to other screens). Log-scaling and depth-normalization is performed on each screen regardless of the `normalize_name` parameter. For example, after normalization T0 columns in `df` will contain log-scaled, depth-normalized read counts, whereas columns from later timepoints will contain depth-normalized LFCs compared to their respective T0s. 

This function additionally removes guides that are lowly-expressed or highly-expressed at T0. To set screens to filter by, pass a list of their names to the `filter_names` parameter. For typical experiments we recommend you pass T0 screens to this parameter. To set minimum and maximum read count thresholds for guides in the T0 screens, pass numbers to the `min_reads` (default 30) and `max_reads` (default 10,000) parameters. Guides will not typically exceed the `max_reads` threshold, but if they do, it is likely a red flag for screen quality.

```{r}
df <- normalize_screens(df, screens, filter_names = c("HAP1_T0", "RPE1_T0"), min_reads = 20)
```

The final set of QC plots that we need to make consists of scatterplot comparisons between each pair of technical replicates. To make these, we call `plot_rep_comparisons`. While it is possible and sometimes could be a good idea to make these before computing LFCs, these results tend to be more consistent after computing LFCs due to depth-normalization, so we recommend making them at this point in the pipeline. Although this function also outputs Pearson correlations between all technical replicates, like all QC plots, manual inspection for potential problematic outlier guides or other red flags or is recommended.

```{r}
plot_screen_reads(df, screens, qc_folder)
```

Before we can parse the data into a structure more suitable for scoring, we need to ensure that we use a common symbol for intergenic regions in both of the gene name columns. In this dataset, intergenic regions are denoted by the symbol "---", which we will change to "None".

```{r}
ind <- df$Gene.symbol1 == "---"
if (sum(ind)) { df$Gene.symbol1[ind] <- "None" }
ind <- df$Gene.symbol2 == "---"
if (sum(ind)) { df$Gene.symbol2[ind] <- "None" }
```

The last thing we need to do before scoring data is parse it into a different structure and split guides by their type. The reason we do this involves the core assumption that orientation (whether Cas9 targets gene A and Cas12a targets gene B, or vice versa) matters for scoring CHyMErA data. Because of this, to make scoring easier for all gene pairs we want to group their guides across each orientation together into a single list ordered by guide IDs. To parse this data, we call the following functions in order:

1. `unique_gene_pairs` gets all unique gene pairs in the dataset, passing in the names of the two gene name columns in the dataset
2. `retrieve_guides_by_label` gets all guides associated with unique gene pairs for every screen according to given gene labels in the columns "Cas9.Guide.Type" and "Cpf1.Guide.Type". These labels must be one of "exonic", "intergenic" or "NT" for non-targeting controls. This also appends guide sequences to use as guide IDs (necessary for scoring data with moderated t-testing downstream). The order of parameters matters here - in the code below, "Cas9.Guide.Type" and "Cas9.Guide" are mapped to genes in "Gene.symbol1", and similarly for the other parameters mapped to "Gene.symbol2".
3. `split_guides_by_type` splits all guides according to whether they target the same gene twice ("single_gene_dual_targeted"), a single gene and an intergenic region ("exonic_intergenic"), or two different genes ("exonic_exonic"). Different types of guides necessitate different scoring methods, which are explained in more detail below. 

```{r}
# Gets gene names and guide counts
gene_pairs <- unique_gene_pairs(df, "Gene.symbol1", "Gene.symbol2")
guides <- retrieve_guides_by_label(df, gene_pairs, screens, 
                                   "Gene.symbol1", "Gene.symbol2", 
                                   "Cas9.Guide.Type", "Cpf1.Guide.Type",
                                   "Cas9.Guide", "Cpf1.Guide")

# Separates single-targeting and dual-targeting guides
temp <- split_guides_by_type(guides)
dual <- temp[["single_gene_dual_targeted"]]
single <- temp[["exonic_intergenic"]]
paralogs <- temp[["exonic_exonic"]]
```

### Dual-targeted scoring

Now that we've processed our data, we need to score it. Thankfully, chymeraR makes this step relatively painless. The most important thing to remember during scoring is that different types of guides need to be scored against different null models. Here, we will detail how to **score data in one or more conditions against a control condition.**

The data in our library that lends itself to this type of comparison is the dual-targeting data contained in the `dual` object. We want to compare untreated screens to screens treated with Torin1. To do this, we pass the data, screens, screen names we want to compare to each other (the control screen name comes first), and the type of testing we want to run into the `score_conditions_vs_control` function. We need to do this twice because we have two separate controls to compare against, although we can pass any number of condition names as a list into the fourth argument of the function. E.g. if we had another drug screen for T12, "BTZ_T12", we could instead pass `c("Torin_T12", "BTZ_T12")` into the first function call below.

By default the chosen test is moderated-t with loess correction on residuals enabled, for improved sensitivity. Although chymeraR supports other types of testing, which are documented in the package, because they are less sensitive we do not recommend them at this time.

``` {r}
dual_scores1 <- score_conditions_vs_control(dual, screens, "HAP1_T12", "Torin_T12", 
                                            test = "moderated-t", loess = TRUE)
dual_scores2 <- score_conditions_vs_control(dual, screens, "HAP1_T18", "Torin_T18", 
                                            test = "moderated-t", loess = TRUE)
```

After scoring data, we want to call hits according to user-specific thresholds on FDR and effect size, naming negative effects and positive effects according to the type of screen performed, using the `call_significant_response` function. The effect size threshold, `differential_threshold`, is based on the absolute value of the effect. Here, only guides with abs(effect) < 0.5 and fdr < 0.1 will be called as significant hits.

```{r}
dual_scores1 <- call_significant_response(dual_scores1, "HAP1_T12", "Torin_T12",
                                          neg_type = "Sensitizer", pos_type = "Suppressor",
                                          fdr_threshold = 0.1, differential_threshold = 0.5)
dual_scores2 <- call_significant_response(dual_scores2, "HAP1_T18", "Torin_T18",
                                          neg_type = "Sensitizer", pos_type = "Suppressor",
                                          fdr_threshold = 0.1, differential_threshold = 0.5)
```

Finally, to finish scoring this data we will generate pretty plots describing differential effects and save both those and the scored data to file. Since these plotting functions only return one plot, they return a ggplot object instead of directly saving to file.

```{r}
# Plots drug response
p <- plot_significant_response(dual_scores1, "HAP1_T12", "Torin_T12")
ggsave(file.path(output_folder, "torin_vs_hap1_t12.png"), width = 10, height = 7, dpi = 300)
p <- plot_significant_response(dual_scores2, "HAP1_T18", "Torin_T18")
ggsave(file.path(output_folder, "torin_vs_hap1_t18.png"), width = 10, height = 7, dpi = 300)

# Writes data to file
write.table(dual_scores1, file.path(output_folder, "dual_targeted_gene_calls_t12.tsv"), sep = "\t",
            row.names = FALSE, col.names = TRUE, quote = FALSE)
write.table(dual_scores2, file.path(output_folder, "dual_targeted_gene_calls_t18.tsv"), sep = "\t",
            row.names = FALSE, col.names = TRUE, quote = FALSE)
```

### Paralog scoring

Scoring paralog data is very similar to scoring dual-targeted data, but requires the use of a separate null model that derives expected effects from single-gene knockout effects. Accordingly, a different set of functions with a nearly identical interface is used to **score data against a derived null model.** 

But first, because we aren't interested in scoring the effects of non-targeting guides and because this could potentially drastically slow the scoring process, we will remove non-targeting guides from the data.

```{r}
nt_ind <- unlist(lapply(paralogs, function(x) x[["gene1"]] == "NT" | x[["gene2"]] == "NT"))
paralogs <- paralogs[!nt_ind]
```

Scoring the data proceeds in much the same way as for conditions against a control. The only major differences here are that we also pass in single-targeting guides to derive a null model from as well as a list of screens to score separately (with no associated control).

```{r}
screens_to_score <- c("HAP1_T12", "HAP1_T18", "RPE1_T18", "RPE1_T24", "Torin_T12", "Torin_T18")
paralog_scores <- score_combn_vs_single(paralogs, single, screens, screens_to_score, test = "moderated-t")
paralog_scores <- call_significant_response_combn(paralog_scores, screens_to_score,
                                                  neg_type = "Negative GI", pos_type = "Positive GI")
```

We then make output plots for all screens.

```{r}
p <- plot_significant_response_combn(paralog_scores, "HAP1_T12", loess = TRUE)
ggsave(file.path(output_folder, "paralog_hap1_t12.png"), width = 10, height = 7, dpi = 300)
p <- plot_significant_response_combn(paralog_scores, "HAP1_T18", loess = TRUE)
ggsave(file.path(output_folder, "paralog_hap1_t18.png"), width = 10, height = 7, dpi = 300)
p <- plot_significant_response_combn(paralog_scores, "RPE1_T18", loess = TRUE)
ggsave(file.path(output_folder, "paralog_rpe1_t18.png"), width = 10, height = 7, dpi = 300)
p <- plot_significant_response_combn(paralog_scores, "RPE1_T24", loess = TRUE)
ggsave(file.path(output_folder, "paralog_rpe1_t24.png"), width = 10, height = 7, dpi = 300)
```

Even though paralog data is not scored against a control, we still want to compare treated screens against untreated screens in some way. To do that visually, we will also make plots that exclude hits which are significant in a given list of control screens. 

```{r}
p <- plot_significant_response_combn(paralog_scores, "Torin_T12", filter_name = "HAP1_T12", loess = TRUE)
ggsave(file.path(output_folder, "paralog_torin_t12.png"), width = 10, height = 7, dpi = 300)
p <- plot_significant_response_combn(paralog_scores, "Torin_T18", filter_name = "HAP1_T18", loess = TRUE)
ggsave(file.path(output_folder, "paralog_torin_t18.png"), width = 10, height = 7, dpi = 300)
```

Finally, we save our scored data to file.

```{r}
write.table(paralog_scores, file.path(output_folder, "paralog_gene_calls.tsv"), sep = "\t",
            row.names = FALSE, col.names = TRUE, quote = FALSE)
```

## Miscellaneous

### Differences from published scoring

There are a number of differences between the scoring workflow presented in this vignette and the relatively naive workflow used to score data in Gonatopoulos-Pournatzis et al. The most important differences include:

* Swapping from Wilcoxon rank-sum hypothesis testing to more sensitive moderated t-testing
* Loess-normalizing residuals before performing hypothesis testing
* Accurately matching guides based on guide IDs to compute residual effects
* Addition of read count-based guide filtering 
